
<!DOCTYPE html>
<!--[if lt IE 7]>      <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]>         <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]>         <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->
	<head>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-119541534-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-119541534-1');
</script>
		
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>Predictive Maintenance with Snowflake and Machine Learning - A Blog Series - Part 2</title>
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="description" content="blog page for Patterson Consulting" />
	<meta name="keywords" content="blog, patterson consulting, deep learning, machine learning, apache hadoop, apache spark, etl, consulting, model search, petting zoo, open ai, computer vision" />
	<meta name="author" content="Patterson Consulting" />

  	<!-- Facebook and Twitter integration -->
	<meta property="og:title" content="Applied Machine Learning Quarterly (Q2 2021)"/>
	<meta property="og:image" content="http://www.pattersonconsultingtn.com/blog/images/meta_og_images/applied_ml_quarterly_og_meta_card_q2_2021.png"/>
	<meta property="og:url" content="http://www.pattersonconsultingtn.com/blog/applied_ml_quarterly_q2_2021.html"/>
	<meta property="og:site_name" content=""/>
	<meta property="og:description" content="This is an ongoing curated list of interesting new open source machine learning models, tools, and visualizations for 2021."/>
	

	<meta name="twitter:title" content="Applied Machine Learning Quarterly (Q2 2021)" />
	<meta data-rh="true" property="twitter:description" content="This is an ongoing curated list of interesting new open source machine learning models, tools, and visualizations for 2021."/>

	<meta name="twitter:image" content="http://www.pattersonconsultingtn.com/blog/images/meta_og_images/applied_ml_quarterly_og_meta_card_q2_2021.png" />
	<meta name="twitter:url" content="http://www.pattersonconsultingtn.com/blog/applied_ml_quarterly_q2_2021.html" />
	<meta name="twitter:card" content="summary_large_image" />

	<!-- Place favicon.ico and apple-touch-icon.png in the root directory -->
	<!-- <link rel="shortcut icon" href="favicon.ico"> -->
	
	<link rel="stylesheet" href="../css/animate.css">
	<link rel="stylesheet" href="../css/bootstrap.css">
	<link rel="stylesheet" href="../css/icomoon.css">

	<link rel="stylesheet" href="../css/owl.carousel.min.css">
	<link rel="stylesheet" href="../css/owl.theme.default.min.css">

	<link rel="stylesheet" href="../css/style.css">

	<link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">

	<link rel="shortcut icon" href="http://www.pattersonconsultingtn.com/pct.ico" type="image/x-icon" />

	<style>
		a { 
			color: #FF0000; 
			text-decoration: underline;
		}

		span.quote_to_rewrite {
			color: #FF0000;
			font-style: italic;
		}

table {
  font-family: arial, sans-serif;
  border-collapse: collapse;
  width: 100%;
}

td, th {
  border: 1px solid #dddddd;
  text-align: left;
  padding: 8px;
}

tr:nth-child(even) {
  background-color: #dddddd;
}

h2 {
	color: #555555;
}

pre {
    background: #f4f4f4;
    border: 1px solid #ddd;
    border-left: 3px solid #f36d33;
    color: #666;
    page-break-inside: avoid;
    font-family: monospace;
    font-size: 15px;
    line-height: 1.6;
    margin-bottom: 1.6em;
    max-width: 100%;
    overflow: auto;
    padding: 1em 1.5em;
    display: block;
    word-wrap: break-word;
}

.news_item_row {
	border: 0px solid #999999; 
	padding: 0px; 
	padding-top: 20px; 
	padding-bottom: 24px; 
	margin: 0px; 
	margin-bottom: 6px; 
	background-color: #ffffff;

}

.news_item_label {
	border: 1px solid #cccccc; 
	border-bottom: 0px; 
	width: 50%; 
	padding: 12px; 
	padding-top: 18px; 
	margin: 0px; 
	margin-left: 0px; 
	background-color: #dddddd;
}


.news_item_body {
	border: 2px solid #cccccc; 
	padding: 12px; 
	padding-top: 18px; 
	margin: 20px; 
	margin-left: 0px; 
	margin-top: 0px; 
	background-color: #ffffff;

}

</style>	

	<script src="../js/modernizr-2.6.2.min.js"></script>
	<!--[if lt IE 9]>
	<script src="js/respond.min.js"></script>
	<![endif]-->

	</head>
	<body class="boxed">
	<!-- Loader -->
	<div class="fh5co-loader"></div>

	<div id="wrap">

	<div id="fh5co-page">
		<header id="fh5co-header" role="banner">
			<div class="container">
				<a href="#" class="js-fh5co-nav-toggle fh5co-nav-toggle dark"><i></i></a>
				<div id="fh5co-logo"><a href="index.html"><img src="../images/website_header_top_march2018_v0.png" ></a></div>
				<nav id="fh5co-main-nav" role="navigation">
		          <ul>
		            
		            <li class="has-sub">
		              <div class="drop-down-menu">
		                <a href="#">Services</a>
		                <div class="dropdown-menu-wrap">
		                  <ul>
		                    
		                    <li><a href="../offerings/data_engineering.html">Data Engineering</a></li>
		                    <li><a href="../offerings/data_science.html">Data Science</a></li>

		                    <li><a href="../offerings/cloud_operations.html">Cloud Operations and Engineering</a></li>
		                    
		                    <li><a href="../offerings/managed_kubeflow.html">Managed Kubeflow</a></li>

		                    <li><a href="../offerings/managed_kafka.html">Managed Kafka</a></li>

		                    <li><a href="../offerings/research_partnerships.html">Research Partnerships</a></li>
		                    
		                  </ul>
		                </div>
		              </div>
		            </li>
		            
		            <li><a href="../partners.html">Partners</a></li>

		            <li><a href="../blog/blog_index.html">Blog</a></li>
		          
		            <li class="cta"><a href="../contact.html">Contact</a></li>
		          </ul>
		        </nav>
			</div>
		</header>
		<!-- Header -->

		
		<div id="fh5co-intro" class="fh5co-section">
			<div class="container">


		
				<div class="row row-bottom-padded-sm">
					<div class="col-md-12" id="fh5co-content">
						<h1>Applied Predictive Maintenance</h1>
						<p>
							<h3>Part 2 of 6: "Sensor Data Ingest, Storage, and Analysis with Snowflake"</h3>

						</p>
						<p>
							Author: Josh Patterson<br/>
							Date: Sept 2021
						</p>
						

						<p>
							Other entries in this series:

							<ul>

								<li>Part 1: <a href="predictive_maintenance_w_snowflake_ml_part_1_biz.html">Making the Business Case for Predictive Maintenance</a></li>
								<li>Part 2: <a href="predictive_maintenance_w_snowflake_ml_part_2_ingest.html">Sensor Data Ingest, Storage, and Analysis with Snowflake</a></li>
								<li>Part 3: <a href="predictive_maintenance_w_snowflake_ml_part_3_eda.html">Exploratory Data Analysis</a></li>
								<li>Part 4: <a href="predictive_maintenance_w_snowflake_ml_part_4_modeling.html">Machine Learning Workflows with Sci-Kit Learn to Build Predictive Maintenance Models</a></li>
								<li>Part 5: <a href="predictive_maintenance_w_snowflake_ml_part_5.html">Analyzing the Results</a></li>
								<li>Part 6: <a href="predictive_maintenance_w_snowflake_ml_part_6.html">Going to Production with Snowpark</a></li>

							</ul>


						</p>





					</div>

				</div>




				<!-- Section 1 of 5: Intro, PM in Cloud -->

				<div class="row row-bottom-padded-sm">
					<div class="col-md-12" id="fh5co-content">

						<h1>Managing Predictive Maintenance Data with the Cloud and Snowflake</h1>

						<p>
							In our last article The ACME Tool Co. team collectively established their goals and business constraints for their predictive maintenance pilot program.

						</p>
						<p>
							In part 2 of this series, ACME Tool Co. data science (ATDS) team starts their journey managing and analyzing the machine failure sensor data.

						</p>						
						<p>
							Key Take Aways:
							<ol>
								<li>Introduction to the Snowflake Cloud Analytics Platform</li>
								<li>How to ingest data into Snowflake tables</li>
								<li>Connecting Google Colab Jupyter Notebooks to Snowflake</li>

							</ol>

						</p>

						
						<p>The ATDS team knows what their requirements are, now let's introduce the sensor dataset the company provided the ATDS team for the predictive maintenance pilot project.</p>


					</div>
				</div>

				<!-- Section 1 of 5: Intro, PM in Cloud -->




				<!-- Section 2 of 5: UCI Dataset -->

				<div class="row row-bottom-padded-sm">
					<div class="col-md-12" id="fh5co-content">


						<h1>The Predictive Maintenance Dataset</h1>

						<p>
							The dataset we will use for this post is the <a href="https://archive.ics.uci.edu/ml/datasets/AI4I+2020+Predictive+Maintenance+Dataset">AI4I 2020 Predictive Maintenance Dataset Data Set</a> from the UCI Machine Learning Repository. 


						</p>

						<p>
							The associated publication released with the dataset goes on to explain their rationale for using synthetic data:

						</p>


						<blockquote class="w3-panel w3-leftbar w3-light-grey" style="padding: 16px; font-size: 12px;">
						  <p style="font-size: 14px;"><i>
						  	"Since real predictive maintenance datasets are generally difficult to obtain and in particular difficult to publish, we present and provide a synthetic dataset that reflects real predictive maintenance encountered in industry to the best of our knowledge."

						  	

						  </i></p>

						  <p>"Explainable Artificial Intelligence for Predictive Maintenance Applications", Stephan Matzka, Third International Conference on Artificial Intelligence for Industries (AI4I 2020), 2020 (in press)</p>
						</blockquote>	

						<p>
							Likewise, for our ACME Tool Co. blog series, we will use this dataset for analyis and modeling as it is available yet realistic. It's worth noting that this data is an aggregate of sensor data over time per machine. If we were working with the raw logs of sensor data then before modeling (if we desired a tabular data structure) we'd run some type of aggregation query across the logs to build a similar aggregate form of the sensor data. In this way, this aggregated form of sensor data is an appropriate and realistic dataset to work with in this scenario.

						</p>

						<p>

							Right now we know little about what the data contains. Before we can do any data exploration (before any modeling work), we need to get some data management and analysis tooling up and running. Let's start with getting our data into an analytic platform that can handle our aggregated data or could continuously ingest sensor data if we had that.

						</p>





					</div>

				</div>


				<!-- Section 2 of 5: UCI Dataset -->






				<!-- Section 3 of 5: Snowflake bulk load -->

				<div class="row row-bottom-padded-sm">
					<div class="col-md-12" id="fh5co-content">
						


						<h1>Snowflake as the Sensor Data Management Platform</h1>

						<img src="./images/snowflake_logo_wide_sept_2021.png" width="400px;" style="float: right;"/>

						<p>
							In part 1 of this blog series the ACME Tool Co management team made it clear they had some constraints for the pilot:

							<ul>
								<li>Dont stand up new hardware</li>
								<li>need to move quickly</li>
								<li>wants a platform for analysis that is cost-effective but scalable</li>

							</ul>

						</p>

						<p>
							The ACME Tool Co. pilot project needs to move fast to prove to the line of business that they can hit our ROI metrics in our operational contract. 

						</p>
						<p>
							This also means the the team needs to find a data platform in the cloud (because they can't bring in new on-premise hardware right now) that allows them to quickly prototype the pilot components while being scalable and cost-effective if the pilot program becomes a longer-term production system.
						</p>


						<p>
							These constraints and timeline make the <a href="https://www.snowflake.com/">Snowflake Cloud Data Warehouse</a> a great fit for data management and analytics components in this project.

						</p>
<!--
						<p>


							Working in the cloud with Snowflake also means we don't have to deal with procuring hardware, software, database configuration, nor writing ETL logic. We can focus on the analytical and data science aspect of our pilot project which makes us more likely to be successful.

						</p>
-->
									<!-- --------------------------------- START: Sidebar --------------------------------- -->
									<div style="float: right; width: 612px; border: 1px solid; padding: 12px; padding-left: 18px; margin-left: 12px; font-size: 12px; background-color: #eeeeee;">

<iframe width="560" height="315" src="https://www.youtube.com/embed/xojAXXRo_S0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>


									</div>


						<p>
							Traditional analytical applications require extensive effort around building out platform infrastructure, which means your team spends a lot of time on non-value generating activities (procuring hardware, software, configuring databases, ETL, etc). These are activities the ACME Tool Co team will not have time nor budget for in this pilot project.

						</p>



						<p>
							Snowflake Cloud Data Warehouse is a cloud-native, fully relational ANSI SQL data warehouse service available in both AWS and Azure
							It's scalability and ability to quickly provision a data warehouse make it the right place to start our sensor data management operation for our pilot program.

						</p>

						
						<p>
							Snowflake has features such as:

							<ul>
								<li>Native JSON Support</li>
								<li>Aggregation Using Streams and Tasks</li>
								<li>Time-series optimized data ingestion with snowpipe</li>
								<li>ability to query data in object storage via external tables</li>

							</ul>

						</p>


						<p>
							
							Snowflake provides us with a rock solid analytical store in the cloud to collect raw machine data over time. Once we need to rebuild our model, we can query the raw data to pull the subset of data we need to build our model.

						</p>

						<p>
							Historically you'd have to wait or request the sensor/machine data to be pre-processed; Snowflake simplifies this aspect of data processing as there is

							no need for data engineering tricks or complex Spark jobs.

						</p>
						<p>

							Given Snowflake's ability to scalably and quickly ingest diverse data sets we don't have to expend a lot of energy and time on building out an ingestion framework. This let's our team focus on data processing to achieve line of business goals more quickly.


							Further, Snowflake only charges for what we use so it provides us with a efficient data ingest mechanism and low-cost data managagement platform for large amounts of machine-generated (e.g., "IoT") data.

						</p>
						



						<p>
							With all of this in mind, let's get started working with Snowflake.
							

						</p>




						<h2>Creating a Predictive Maintenance Database in Snowflake</h2>


						<p>
							To get started with managing our sensor data in Snowflake we need to do the following:

							<ol>
								<li>Sign-up for a Snowflake account (<a href="https://signup.snowflake.com/">free, if you don't already have one</a>)</li>
								<li>Create a database in Snowflake for the project</li>
								<li>Create a table in the database for our sensor data</li>
								<li>Load the sensor data into the table</li>

							</ol>



						</p>


						<p>
							Let's get to signing up for Snowflake first.

						</p>


						<p>
							If you already have a snowflake account head to the site and log in. If you don't have an account, head to <a href="https://signup.snowflake.com/">this link and sign up for the free account</a>.

						</p>

						<p>
							We're going to do everything from the command line via the SnowSQL (CLI Client):

							https://docs.snowflake.com/en/user-guide/snowsql.html

						</p>

					<p>
						Once you have the SnowSQL CLI Client installed, open a terminal window and log into your Snowflake account from the command line with the following command:

					</p>

<pre>
$ snowsql -a [account_name] -u [user_name]
</pre>
						<p>
							This should show console output similar to the output below:

						</p>		

<consoleoutput>jpatanooga#COMPUTE_WH@(no database).(no schema)>
</consoleoutput><br/>		




						<p>
							Now that we have connectivity, let's move on to creating a database and table for our sensor data

						</p>





						<h2>Creating a Database and Table for Our Sensor Data on Snowflake</h2>

						<p>
							Now let's use the SnowSQL client to create a database and a table to manage our data. This tutorial has an associated github repository with the SQL scripts and Jupyter notebooks referenced: <a href="https://github.com/pattersonconsulting/predictive_maintenance">https://github.com/pattersonconsulting/predictive_maintenance</a>

						</p>
						<p>
							For reference, if you have <code>git</code> installed, you can quickly clone the project repository with the following command:

						</p>
<pre>
git clone https://github.com/pattersonconsulting/predictive_maintenance.git
</pre>						

						<p>
							Tp create a database (<code>predictive_maintenance</code>) and table in the database (<code>summary_sensor_data</code>) use the following script (<code>create_pm_db_and_table.sql</code>):

						</p>
<pre>
create or replace database predictive_maintenance;

create or replace table summary_sensor_data (
  UDI int ,
  Product_ID string ,
  Type string ,
  Air_temperature float ,
  Process_temperature float ,
  Rotational_speed float ,
  Torque float ,
  Tool_wear float ,

  Machine_failure int ,

  TWF int ,
  HDF int ,
  PWF int ,
  OSF int ,
  RNF int 
  
  );
</pre>
						<p>
							We can execute the script included in the github repository from the terminal command line with the command:



						</p>

<pre>
snowsql -a [xxxxxxx.us-east-1] -u [user_name] -f ./create_pm_db_and_table.sql 
</pre>						
						<p>
							And we should see output

						</p>

<consoleoutput>* SnowSQL * v1.2.18
Type SQL statements or !help
+-------------------------------------------------------+                       
| status                                                |
|-------------------------------------------------------|
| Database PREDICTIVE_MAINTENANCE successfully created. |
+-------------------------------------------------------+
1 Row(s) produced. Time Elapsed: 0.183s
+-------------------------------------------------+                             
| status                                          |
|-------------------------------------------------|
| Table SUMMARY_SENSOR_DATA successfully created. |
+-------------------------------------------------+
1 Row(s) produced. Time Elapsed: 0.267s
</consoleoutput><br/>

						<p>
							Now let's move the sensor data into our Snowflake table.

						</p>


						<div style="width:900px; margin:0 auto;">

							<div class="w3-panel w3-leftbar w3-light-grey" style="padding: 16px; font-size: 12px; border: 1px solid #999999; width: 85%; text-align: left;">
								<img src="./images/spyglass_icon.jpg" style=" width: 80px; height: 80px; float: left; margin-right: 20px;" />

								<h4>Continuous Sensor Data Ingestion with Snowflake</h4>
							  <p style="font-size: 14px; ">
							  <i>
							  
							  <p>
							It's worth noting that we'd normally use Snowpipe, Apache Kafka, and/or AWS for sensor data ingestion, but for this demo, we're going to bulk load the data as a single CSV file. We'd also use snowflake to create the daily aggregates from the raw sensor data updates for machine learning training.

						</p>
						<p>


							Typically we might see the data being streamed from a system such as Apache Kafka, and then Snowpipe (by Snowflake) allows for a streaming-based data ingest approach. This allows for micro-batch processing of the data as well which best supports the underlying distributed computations in the Snowflake parallel data warehouse.
						</p>

							<p>
							If you'd like to know more about Snowpipe check out the <a href="https://docs.snowflake.com/en/user-guide/data-load-snowpipe.html">documentation</a> or check out the <a href="https://developers.snowflake.com/wp-content/uploads/2020/09/SNO-e-Book-20004_SNO-eBook-7-Reference-Architectures-for-Application-Builders-IoT.pdf">Snowflake IoT Reference Architecture</a>.

			

						</p>

			
							</i></p>
							  
							</div>	

						</div>


						<h2>Copy Predictive Maintenance Dataset into Snowflake</h2>


						<p>
							Download the UCI dataset from the following URL:

						</p>
						<p>
							<a href="https://archive.ics.uci.edu/ml/datasets/AI4I+2020+Predictive+Maintenance+Dataset">AI4I 2020 Predictive Maintenance Dataset Data Set</a> (from the UCI Machine Learning Repository).

						</p>

						<p>

							We now we load this raw CSV sensor data into Snowflake. For reference, check out the Snowflake documentation on "Loading data into Snowflake":
						</p>
						<p>

							

							<a href="https://docs.snowflake.com/en/user-guide-data-load.html">https://docs.snowflake.com/en/user-guide-data-load.html</a>


						</p>


						<p>
							If we change our current datasbase to <code>PREDICTIVE_MAINTENANCE</code> the output should look like:

							</p>



<consoleoutput>jpatanooga#COMPUTE_WH@(no database).(no schema)>use PREDICTIVE_MAINTENANCE;
+----------------------------------+                                            
| status                           |
|----------------------------------|
| Statement executed successfully. |
+----------------------------------+
1 Row(s) produced. Time Elapsed: 0.386s
jpatanooga#COMPUTE_WH@PREDICTIVE_MAINTENANCE.PUBLIC>
</consoleoutput><br/>

						<p>
							Now we can ask Snowflake to <code>show tables;</code> and it will output:

						</p>

<consoleoutput>show TABLES;
+-------------------------------+---------------------+------------------------+-------------+-------+---------+------------+------+-------+----------+----------------+----------------------+-----------------+-------------+
| created_on                    | name                | database_name          | schema_name | kind  | comment | cluster_by | rows | bytes | owner    | retention_time | automatic_clustering | change_tracking | is_external |
|-------------------------------+---------------------+------------------------+-------------+-------+---------+------------+------+-------+----------+----------------+----------------------+-----------------+-------------|
| 2021-09-21 12:15:32.144 -0700 | SUMMARY_SENSOR_DATA | PREDICTIVE_MAINTENANCE | PUBLIC      | TABLE |         |            |    0 |     0 | SYSADMIN | 1              | OFF                  | OFF             | N           |
+-------------------------------+---------------------+------------------------+-------------+-------+---------+------------+------+-------+----------+----------------+----------------------+-----------------+-------------+
1 Row(s) produced. Time Elapsed: 0.149s

</consoleoutput><br/>			

							<p>
							We can get more detail on our table <code>RAW_DEVICE_DATA</code> with the <code>describe table</code> command:

							</p>
<consoleoutput>describe table SUMMARY_SENSOR_DATA;
+---------------------+-------------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+
| name                | type              | kind   | null? | default | primary key | unique key | check | expression | comment | policy name |
|---------------------+-------------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------|
| UDI                 | NUMBER(38,0)      | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |
| PRODUCT_ID          | VARCHAR(16777216) | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |
| TYPE                | VARCHAR(16777216) | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |
| AIR_TEMPERATURE     | FLOAT             | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |
| PROCESS_TEMPERATURE | FLOAT             | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |
| ROTATIONAL_SPEED    | FLOAT             | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |
| TORQUE              | FLOAT             | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |
| TOOL_WEAR           | FLOAT             | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |
| MACHINE_FAILURE     | NUMBER(38,0)      | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |
| TWF                 | NUMBER(38,0)      | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |
| HDF                 | NUMBER(38,0)      | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |
| PWF                 | NUMBER(38,0)      | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |
| OSF                 | NUMBER(38,0)      | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |
| RNF                 | NUMBER(38,0)      | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |
+---------------------+-------------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+
14 Row(s) produced. Time Elapsed: 0.436s
</consoleoutput><br/>



						<p>
							Put the data in an internal staging table on snowflake

Video: Explain how the implied stage table is addressed via syntax (%)
View the staged data
Copy the staged data into the target table


						</p>


						<p>Each table has a Snowflake stage allocated to it by default for storing files (when using "Table Stage", <a href="https://docs.snowflake.com/en/user-guide/data-load-local-file-system-stage.html">check out the staging documentation</a>).
							We will use the associated staging table (<code>@predictive_maintenance.public.%SUMMARY_SENSOR_DATA</code>) to load the data into Snowflake. To stage the sensor data in Snowflake for loading use the following command (referencing the sql script <code>stage_sensor_data.sql</code> included):


						</p>

<pre>
snowsql -a nna57244.us-east-1 -u jpatanooga -f ./stage_sensor_data.sql     
</pre>

				<p>
					Depending on where you downloaded the UCI sensor data csv file you have need to update the staging script sql. Once the command is run, you should see output similar to the output below.

				</p>

<consoleoutput>
ai4i2020.csv_c.gz(0.13MB): [##########] 100.00% Done (0.506s, 0.25MB/s).        
+--------------+-----------------+-------------+-------------+--------------------+--------------------+----------+---------+
| source       | target          | source_size | target_size | source_compression | target_compression | status   | message |
|--------------+-----------------+-------------+-------------+--------------------+--------------------+----------+---------|
| ai4i2020.csv | ai4i2020.csv.gz |      522048 |      134490 | NONE               | GZIP               | UPLOADED |         |
+--------------+-----------------+-------------+-------------+--------------------+--------------------+----------+---------+
1 Row(s) produced. Time Elapsed: 1.288s
Goodbye!                                                   
</consoleoutput><br/>

<p>
	To confirm we staged the file on snowflake use the following <code>list</code> command from SnowSQL:
</p>
<pre>
list @predictive_maintenance.public.%SUMMARY_SENSOR_DATA;
</pre>
					<p>
						You should see something similar to the output below:

					</p>

<consoleoutput>+-----------------+--------+----------------------------------+-------------------------------+
| name            |   size | md5                              | last_modified                 |
|-----------------+--------+----------------------------------+-------------------------------|
| ai4i2020.csv.gz | 134496 | 8871341f84e4f56a7f689f1ac8b32fba | Tue, 21 Sep 2021 19:48:22 GMT |
+-----------------+--------+----------------------------------+-------------------------------+
1 Row(s) produced. Time Elapsed: 0.410s
</consoleoutput><br/>

					<p>
						Now let's copy the staged data into the target table. We'll use the included script that has the following contents:

					</p>
<pre>
use database PREDICTIVE_MAINTENANCE;

COPY INTO summary_sensor_data
From @%summary_sensor_data
FILE_FORMAT = ( TYPE = CSV, SKIP_HEADER=1 )

</pre>

						<p>
							Run this script with the command:

						</p>
<pre>
snowsql -a nna57244.us-east-1 -u jpatanooga -f ./copy_staged_data_to_table.sql
</pre>
						<p>

							And the output should look like:

						</p>						

<consoleoutput>+----------------------------------+                                            
| status                           |
|----------------------------------|
| Statement executed successfully. |
+----------------------------------+
1 Row(s) produced. Time Elapsed: 0.099s
+-----------------+--------+-------------+-------------+-------------+-------------+-------------+------------------+-----------------------+-------------------------+
| file            | status | rows_parsed | rows_loaded | error_limit | errors_seen | first_error | first_error_line | first_error_character | first_error_column_name |
|-----------------+--------+-------------+-------------+-------------+-------------+-------------+------------------+-----------------------+-------------------------|
| ai4i2020.csv.gz | LOADED |       10000 |       10000 |           1 |           0 | NULL        |             NULL |                  NULL | NULL                    |
+-----------------+--------+-------------+-------------+-------------+-------------+-------------+------------------+-----------------------+-------------------------+
1 Row(s) produced. Time Elapsed: 0.787s
</consoleoutput><br/>

						<p>
							Now if we log into the SnowSQL console and run the query:

						</p>

<pre>
select * from SUMMARY_SENSOR_DATA limit 5;
</pre>
						<p>We should see:</p>

<consoleoutput>+-----+------------+------+-----------------+---------------------+------------------+--------+-----------+-----------------+-----+-----+-----+-----+-----+
| UDI | PRODUCT_ID | TYPE | AIR_TEMPERATURE | PROCESS_TEMPERATURE | ROTATIONAL_SPEED | TORQUE | TOOL_WEAR | MACHINE_FAILURE | TWF | HDF | PWF | OSF | RNF |
|-----+------------+------+-----------------+---------------------+------------------+--------+-----------+-----------------+-----+-----+-----+-----+-----|
|   1 | M14860     | M    |           298.1 |               308.6 |             1551 |   42.8 |         0 |               0 |   0 |   0 |   0 |   0 |   0 |
|   2 | L47181     | L    |           298.2 |               308.7 |             1408 |   46.3 |         3 |               0 |   0 |   0 |   0 |   0 |   0 |
|   3 | L47182     | L    |           298.1 |               308.5 |             1498 |   49.4 |         5 |               0 |   0 |   0 |   0 |   0 |   0 |
|   4 | L47183     | L    |           298.2 |               308.6 |             1433 |   39.5 |         7 |               0 |   0 |   0 |   0 |   0 |   0 |
|   5 | L47184     | L    |           298.2 |               308.7 |             1408 |   40   |         9 |               0 |   0 |   0 |   0 |   0 |   0 |
+-----+------------+------+-----------------+---------------------+------------------+--------+-----------+-----------------+-----+-----+-----+-----+-----+
5 Row(s) produced. Time Elapsed: 0.226s
</consoleoutput><br/>


						<div style="width:900px; margin:0 auto;">

							<div class="w3-panel w3-leftbar w3-light-grey" style="padding: 16px; font-size: 12px; border: 1px solid #999999; width: 85%; text-align: left;">
								<img src="./images/spyglass_icon.jpg" style=" width: 80px; height: 80px; float: left; margin-right: 20px;" />

								<h4>Data Load, Ingest, and Queries with Snowflake</h4>
							  <p style="font-size: 14px; ">
							  <i>In the "olden days" (as my kids say) it was a challenge to build out ingest pipleines for systems such as Hadoop and then manage how new data came in while running queries against the data being changed. A great aspect of the Snowflake platform is that we can write to a logical table all day long and we don't have to manage the updates or which temporary copy of the data is being processed -- it just all happens behind the scenes.</i></p>
							  
							</div>	

						</div>


<!--
					<div style="width:900px; margin:0 auto;">

						<div class="w3-panel w3-leftbar w3-light-grey" style="padding: 16px; font-size: 12px; border: 1px solid #999999; width: 85%; text-align: left;">
							<img src="./images/crab_warn_230.png" style=" width: 80px; height: 80px; float: left; margin-right: 20px;" />
							<div style="border: solid 0px; overflow: auto;">

							<h4><code>rtree</code> is Challenging to Install</h4>
						  <p style="font-size: 14px; ">
						  <i><code>rtree</code> will not install via pip and the normal requirements.txt  dependencies. It will conflict with multiple other methods of install (depending on your platform) via a normal <code>pip</code>, so to get it working we need to use <code>apt</code> to install a key dependency (<code>libspatialindex-dev</code>) and build a custom docker image. Otherwise we could use the standard docker image for Kubeflow notebooks and load the dependencies at runtime in the notebook.</i></p>

						</div>


						</div>	
					</div>		
-->					
						<p>Now that we have established our cloud-base sensor data management platform and loaded our data, let's connect some data science tools to snowflake.</p>






					</div>

				</div>

				<!-- Section 3 of 5: Snowflake bulk load -->




				<!-- Section 4 of 5: Connecting Google Co-lab to Snowflake -->
				<div class="row row-bottom-padded-sm">
					<div class="col-md-12" id="fh5co-content">

						<h1>Google Colaboratory ("Google Colab") as EDA and Data Science Platform</h1>

						<p>
							<a href="https://colab.research.google.com/notebooks/intro.ipynb?utm_source=scs-index">Google Colaboratory</a> is a cloud-based Jupyter notebook server that is free to use. Given that we have a small team of data scientists that want to use python in Jupyter notebooks and we need to run notebooks in the cloud, its a great option for the ACME Tool Co. data science team to "move fast".

						</p>


						<p>
							The ACME Tool Co. data science team needs to explore the sensor data before any modeling occurs and has some existing exploratory data analysis methods they want to use to analyze the data before modeling. Fortunately there is a Snowflake python connector and it works from inside Google Colab, keeping all of our analysis and data management in the cloud.

						</p>

						
						<h2>Connecting to Snowflake from Google Colab</h2>

						<p>The nice thing about the Snowflake Connector for Python is that we can just <code>pip</code> install it in the Colab notebook and keep on moving via:</p>

<pre>
!pip install snowflake-connector-python						
</pre>

						<p>Once we do that, we can quickly test our connection to our Snowflake account with the following code:</p>

<pre>
import snowflake.connector

# Gets the version
ctx = snowflake.connector.connect(
    user='[your_account_here]',
    password='[your_pw_here],
    account='[xxxxxxxxxx.us-east-1]'
    )
cs = ctx.cursor()
try:
    cs.execute("SELECT current_version()")
    one_row = cs.fetchone()
    print(one_row[0])
finally:
    cs.close()
ctx.close()
</pre>


						<p>At this point the ACME Tool Co. data science (ATDS) team has a platform they can ingest sensor data into and then pull into a juypter notebook for analytics and machine learning in the cloud.</p>

					</div>

				</div>

				<!-- Section 4 of 5: Connecting Google Co-lab to Snowflake -->




				<!-- Section 5 of 5: Next Steps: EDA -->
				<div class="row row-bottom-padded-sm">
					<div class="col-md-12" id="fh5co-content">
						
						<h1>Next Steps: Ready for Data Exploration</h1>


						<p>


							In this post in our series we learned how Snowflake provides us with a rock solid analytical store in the cloud to collect raw machine data over time. Once the ATDS team needs to rebuild their model, they can query the raw data to build any summary aggregates for analysis and modeling.

						</p>
						<p>
							Before the ATDS team can start building machine learning models they need to better understand what is in the current dataset. This exploratory process is known as "Exploratory Data Analysis" (EDA) and is key to informing how to best start our machine learning activities. In the next post (<a href="predictive_maintenance_w_snowflake_ml_part_3_eda.html">part 3</a>) we are going to dive straight into performing an EDA workflow on the sensor data stored in Snowflake.

						</p>

					</div>

				</div>
				<!-- Section 5 of 5: Next Steps: EDA -->







				<div class="row row-bottom-padded-sm" style=" border: 1px solid #cccccc; border-radius: 10px; padding: 8px; padding-top: 8px; background-color: #FF5126; color: #ffffff; font-size: 14px; font-weight: normal; margin-bottom: 30px; ">
					
					<div class="col-md-3" id="fh5co-content" style="">

						<h3 style="color: #ffffff;">Looking for Analytics Help?</h3>

					</div>
					<div class="col-md-9" id="fh5co-content" style="margin-bottom: 0px;">


						<div style="background-color: ; padding: 1px; margin-bottom: 0px;">
						<p style="margin: 0px;">
							Our team can help -- we help companies with Snowflake analytics.

						</p>
						</div>
					</div>


				</div>		




			</div>
		</div>



		<footer id="fh5co-footer" role="contentinfo">
			<div class="container">
				<div class="row row-bottom-padded-sm">
					<div class="col-md-4 col-sm-12">
					</div>
					<div class="col-md-3 col-md-push-1 col-sm-12 col-sm-push-0">
						<div class="fh5co-footer-widget">
				

						</div>
					</div>
					<div class="col-md-3 col-md-push-2 col-sm-12 col-sm-push-0">
						
						<div class="fh5co-footer-widget">
							<h3>Follow us</h3>
							<ul class="fh5co-social">
								<li class="twitter"><a href="https://twitter.com/PattersonCnsltg"><i class="icon-twitter"></i></a></li>
								<li class="linkedin"><a href="https://www.linkedin.com/company/patterson-consulting-tn"><i class="icon-linkedin"></i></a></li>
								<li class="message"><a href="mailto:josh@pattersonconsultingtn.com"><i class="icon-mail"></i></a></li>
							</ul>
						</div>
					</div>

				</div>

			</div>
		</footer>


	</div>
	</div>

	<div class="gototop js-top">
		<a href="#" class="js-gotop"><i class="icon-chevron-down"></i></a>
	</div>
	
	<script src="../js/jquery.min.js"></script>
	<script src="../js/jquery.easing.1.3.js"></script>
	<script src="../js/bootstrap.min.js"></script>
	<script src="../js/owl.carousel.min.js"></script>
	<script src="../js/main.js"></script>

	</body>
</html>					
