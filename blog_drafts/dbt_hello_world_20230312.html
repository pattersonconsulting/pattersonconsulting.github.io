<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Josh Patterson" />
  <meta name="keywords" content="snowflake, snowpark, automl, AutoGluon, pandas, dataframe, whl, pip, anaconda, dependency" />
  <title>DBT Hello World</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header>
<h1 class="title">DBT Hello World</h1>
<p class="subtitle">Understanding the Basics of Building a Data Pipeline with DBT</p>
<p class="author">Josh Patterson</p>
</header>
<h1 id="introduction-to-dbt">Introduction to DBT</h1>
<p>Other entries in this series:</p>
<h2 id="what-is-dbt">What is DBT?</h2>
<p>DBT (Data Build Tool) is an open-source command-line tool that allows data analysts and engineers to transform and manipulate data in their data warehouse. It provides a framework for creating and executing complex SQL queries and transforms, and it helps manage the entire data transformation process.</p>
<p>DBT is designed to work with a variety of data warehouses, including Snowflake, BigQuery, Redshift, and others. It allows users to define data models and transformations in a modular way, and it supports version control and collaboration through Git.</p>
<p>With DBT, analysts can define reusable SQL code and maintain a consistent data model across their organization. They can also build automated testing and validation into their data pipelines, ensuring data accuracy and consistency. Overall, DBT helps organizations build data pipelines that are maintainable, scalable, and easy to understand.</p>
<h2 id="role-in-analytics">Role in Analytics</h2>
<ul>
<li>why would we use dbt?</li>
</ul>
<p>DBT plays a critical role in an enterprise analytics pipeline by providing a scalable and maintainable way to transform and manipulate data in a data warehouse. Here are some specific ways that DBT can support an analytics pipeline:</p>
<p>Transforming and modeling data: DBT allows data analysts and engineers to define data models and transformations in a modular way using SQL, making it easy to build complex data pipelines. DBT also supports incremental loads, allowing analysts to quickly update their data without having to reprocess the entire dataset.</p>
<p>Data validation and testing: DBT has built-in support for automated testing and validation, allowing analysts to ensure data accuracy and consistency. DBT can automatically run tests on data models and transformations as part of a continuous integration and deployment (CI/CD) pipeline.</p>
<p>Collaboration and version control: DBT integrates with Git, allowing analysts to collaborate on data models and transformations using a familiar version control system. This makes it easy to track changes to data pipelines over time and maintain a consistent codebase.</p>
<p>Documentation and lineage tracking: DBT includes built-in documentation features that allow analysts to document data models, transformations, and pipelines, making it easier for other team members to understand the data and how it’s transformed. DBT also supports lineage tracking, allowing analysts to track data from its source to its destination and ensure data quality.</p>
<p>Overall, DBT provides a scalable and maintainable way to transform and manipulate data in a data warehouse, making it an essential tool for enterprise analytics pipelines.</p>
<h3 id="what-does-dbt-replace">what does DBT replace?</h3>
<p>DBT is not designed to replace any specific tool or technology, but rather to complement and enhance existing data warehousing and analytics tools. DBT can be used alongside ETL tools, such as Apache Airflow or Apache Nifi, and data visualization tools, such as Tableau or Looker, to provide a more complete data analytics solution.</p>
<p>However, DBT does offer some unique features that can replace certain manual or ad-hoc data transformation processes. For example, DBT can replace custom SQL scripts that are used to transform data, as it provides a more modular and reusable way to define data transformations. DBT also provides a more scalable way to transform data compared to traditional ETL tools, which may require significant maintenance and updates as the data pipeline grows in complexity.</p>
<p>Overall, DBT is a powerful tool that can enhance and streamline existing data analytics pipelines, but it’s not designed to replace any specific tool or technology outright.</p>
<h2 id="anatomy-of-a-dbt-project">Anatomy of a DBT Project</h2>
<ul>
<li>core files</li>
<li>directory structure</li>
<li>configuring a dbt project</li>
<li>connecting to databases</li>
</ul>
<p>A DBT project is typically organized into several folders and files, each with a specific purpose. Here is a breakdown of the typical structure of a DBT project:</p>
<ol type="1">
<li><p>Models: This folder contains the SQL code that defines data models and transformations. Each model is typically defined in a separate SQL file.</p></li>
<li><p>Data: This folder contains any data files or SQL scripts needed to support data modeling and transformation.</p></li>
<li><p>Macros: This folder contains reusable SQL code that can be called from data models or transformations.</p></li>
<li><p>Analysis: This folder contains SQL scripts that perform data analysis or generate reports.</p></li>
<li><p>Tests: This folder contains SQL scripts that test the validity of data models or transformations.</p></li>
<li><p>dbt_project.yml: This file defines the configuration settings for the DBT project, such as the target database and credentials, and other project-specific settings.</p></li>
<li><p>profiles.yml: This file contains the database connection information, such as the host, username, password, and database name.</p></li>
<li><p>Documentation: This folder contains documentation generated by DBT about the data models, tests, and macros defined in the project.</p></li>
</ol>
<p>By organizing a DBT project in this way, it becomes easier to manage and maintain data pipelines, collaborate with other team members, and implement automated testing and validation.</p>
<h2 id="core-concepts-in-dbt-data-modeling">Core Concepts in DBT Data Modeling</h2>
<ul>
<li>Data Model – what is it?</li>
<li>[ TODO: Diagram HERE ]</li>
</ul>
<p>A DBT data model is a SQL query that defines a specific view of data in a data warehouse. The data model can include joins, filters, aggregations, and other transformations that shape the data into a format that’s easier to work with for downstream analysis and reporting.</p>
<p>DBT models are defined using SQL syntax and can be composed of other models or database tables. Models are defined in separate SQL files, and DBT provides a mechanism for organizing models into a modular structure.</p>
<p>DBT models are incremental, which means that DBT only processes the new or changed data since the last time the model was run. This makes DBT models more efficient than traditional ETL processes that require processing all of the data each time.</p>
<p>DBT data models are part of a larger data pipeline that transforms raw data into a format that’s more useful for data analysis and reporting. By using DBT to define data models, analysts can build complex data pipelines that are modular, maintainable, and easy to test and validate.</p>
<h1 id="building-our-dbt-hello-world-project-with-synthetic-patient-data">Building Our DBT Hello World Project with Synthetic Patient Data</h1>
<ul>
<li>conda env creation</li>
<li>install duckDB and connector</li>
<li>load raw data into DuckDB</li>
<li>dbt init [project]</li>
<li>configuring connection to snowflake</li>
</ul>
<h2 id="the-synthetic-patient-doctor-visit-data">The Synthetic Patient Doctor Visit Data</h2>
<ul>
<li>[ describe the data here ]</li>
</ul>
<h2 id="create-environment-in-conda">Create Environment in Conda</h2>
<p>To create an environment in Conda, you can follow these steps:</p>
<ol type="1">
<li><p>Open your terminal (or Anaconda Prompt on Windows).</p></li>
<li><p>Type the following command to create a new environment named my_env (you can replace my_env with your preferred name):</p></li>
</ol>
<pre><code>conda create --name my_env</code></pre>
<ol start="3" type="1">
<li><p>Conda will ask you to confirm the installation of any additional packages that are required to create the environment. Type y and press Enter to proceed.</p></li>
<li><p>Once the environment is created, activate it by typing:</p></li>
</ol>
<pre><code></code></pre>
<ol start="5" type="1">
<li><p>Your prompt should now show the name of the environment in parentheses, indicating that it’s active.</p></li>
<li><p>You can now install any packages you need for your project using conda install or pip install.</p></li>
<li><p>When you’re done working in the environment, you can deactivate it by typing:</p></li>
</ol>
<pre><code>conda deactivate</code></pre>
<h2 id="install-dbt-and-duckdb-connector">Install DBT and DuckDB Connector</h2>
<p>[ DuckDB and DBT for this one locally? ]</p>
<pre><code>https://duckdb.org/docs/installation/index

pip3 install dbt-duckdb
</code></pre>
<p>The latest supported version targets dbt-core 1.1.x and duckdb 0.3.2.</p>
<h3 id="using-dbt-locally-vs-using-dbt-in-the-cloud">Using DBT Locally vs Using DBT in the Cloud</h3>
<ul>
<li>need to reference [ exploratory vs enterprise data pipelines here ]</li>
<li>Josh Wills notes on why he used DBT with DuckDB</li>
</ul>
<h3 id="load-some-synthetic-patient-data-into-duckdb">Load Some Synthetic Patient Data Into DuckDB</h3>
<ul>
<li>load the doctor visit csv file into DuckDB</li>
<li>make note of the path</li>
</ul>
<h2 id="configure-dbt-profile-to-connect-to-duckdb-database">Configure DBT Profile to Connect to DuckDB Database</h2>
<p>To configure DBT to connect to a database, you need to create a configuration file called profiles.yml. This file should contain the necessary connection information for each database you want to connect to.</p>
<p>On OSX, the DBT profile.yml file lives at:</p>
<pre><code>cat ~/.dbt/profiles.yml</code></pre>
<h3 id="general-notes-on-editing-profile.yml">General Notes on Editing profile.yml</h3>
<p>Here are the general steps to configure DBT to connect to a database:</p>
<ol type="1">
<li><p>Open a text editor and create a new file called profiles.yml in the root directory of your DBT project.</p></li>
<li><p>Inside the profiles.yml file, define a profile for each database you want to connect to. The profile should include the database type (e.g., postgres, redshift, etc.), host, port, database name, username, and password. Here’s an example profile for a PostgreSQL database:</p></li>
</ol>
<pre><code>my_database:
  target: dev
  outputs:
    dev:
      type: postgres
      host: myhostname.com
      port: 5432
      dbname: mydatabase
      user: myusername
      password: mypassword
      schema: myschema
      threads: 4
      keepalives_idle: 0</code></pre>
<ol start="3" type="1">
<li><p>Save the profiles.yml file.</p></li>
<li><p>In your DBT project, you can now reference the profile name in your dbt_project.yml file. For example, if your profile is called my_database, you can reference it like this:</p></li>
</ol>
<pre><code># dbt_project.yml
...
profile: my_database
...</code></pre>
<ol start="5" type="1">
<li>Once you’ve configured your profiles, you can run DBT commands like dbt run or dbt test to execute your DBT project against the specified database.</li>
</ol>
<p>That’s it! With these steps, you can configure DBT to connect to a database and start building data models and pipelines.</p>
<h3 id="configure-the-profile-for-duckdb">Configure the Profile for DuckDB</h3>
<pre><code>dbt_duckdb_test:
  target: dev
  outputs:
    dev:
      type: duckdb
      path: &#39;...../workspaces/snowpark_demos/duckdb/fips_test_db&#39;
      #optional fields
      #schema: schema_name </code></pre>
<h2 id="initialize-dbt-project">Initialize DBT Project</h2>
<p>From: https://docs.getdbt.com/docs/building-a-dbt-project/projects</p>
<pre><code>dbt init [project_name]</code></pre>
<p>So here we can just use:</p>
<pre><code>dbt init
20:04:41  Running with dbt=1.1.1
20:04:41  Setting up your profile.
The profile dbt_duckdb_test already exists in /Users/josh/.dbt/profiles.yml. Continue and overwrite it? [y/N]: N</code></pre>
<p>And then let’s check out what was generated:</p>
<pre><code>ls -la

total 24
drwxr-xr-x  12 josh  staff   384 Jul 28 16:04 .
drwxr-xr-x   7 josh  staff   224 Jul 28 16:02 ..
-rw-r--r--   1 josh  staff    29 Jul 27 15:00 .gitignore
-rw-r--r--   1 josh  staff   571 Jul 27 15:00 README.md
drwxr-xr-x   3 josh  staff    96 Jul 27 15:00 analyses
-rw-r--r--   1 josh  staff  1349 Jul 28 16:02 dbt_project.yml
drwxr-xr-x   3 josh  staff    96 Jul 28 16:04 logs
drwxr-xr-x   3 josh  staff    96 Jul 27 15:00 macros
drwxr-xr-x   3 josh  staff    96 Jul 27 15:00 models
drwxr-xr-x   3 josh  staff    96 Jul 27 15:00 seeds
drwxr-xr-x   3 josh  staff    96 Jul 27 15:00 snapshots
drwxr-xr-x   3 josh  staff    96 Jul 27 15:00 tests</code></pre>
<h2 id="run-dbt-pipeline">Run DBT Pipeline</h2>
<h3 id="confirm-dbt-profile-connection-works">Confirm DBT Profile Connection Works</h3>
<p>Try the following command:</p>
<pre><code>dbt debug

20:04:52  Running with dbt=1.1.1
dbt version: 1.1.1
python version: 3.8.13
python path: /Users/josh/opt/anaconda3/envs/hurricane_analytics/bin/python
os info: macOS-10.16-x86_64-i386-64bit
Using profiles.yml file at /Users/josh/.dbt/profiles.yml
Using dbt_project.yml file at /Users/josh/Documents/PattersonConsulting/workspaces/snowpark_demos/dbt/dbt_duckdb_test/dbt_project.yml

Configuration:
  profiles.yml file [OK found and valid]
  dbt_project.yml file [OK found and valid]

Required dependencies:
 - git [OK found]

Connection:
  database: main
  schema: main
  path: /Users/josh/Documents/PattersonConsulting/workspaces/snowpark_demos/duckdb/fips_test_db
  Connection test: [OK connection ok]

All checks passed!</code></pre>
<h3 id="run-dbt-pipeline-locally">Run DBT Pipeline Locally</h3>
<pre><code>dbt run</code></pre>
<h3 id="run-dbt-pipeline-from-github">Run DBT Pipeline from Github</h3>
<h1 id="summary">Summary</h1>
<p>In this blog post we gave you an overview of DBT and then showed how to build a simple “Hello World”-style of application. For more information on DBT, check out their documentation. For more information on cloud infrastructure, check out the rest of the articles in our blog.</p>
<p>We also offer private workshops for companies on topics such as creating and running DBT pipelines, please feel free to reach out if you’d like to discuss attending one of our workshops.</p>
</body>
</html>
