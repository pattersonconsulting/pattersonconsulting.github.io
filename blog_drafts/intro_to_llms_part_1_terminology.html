<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Josh Patterson" />
  <meta name="keywords" content="snowflake, snowpark, automl, AutoGluon,
pandas, dataframe, whl, pip, anaconda, dependency" />
  <meta name="description" content="In this post we’ll ….." />
  <title>An Introduction to Large Language Models (LLMs)</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">An Introduction to Large Language Models (LLMs)</h1>
<p class="subtitle">Part 1 - Core Concepts and Terminology in LLMs</p>
<p class="author">Josh Patterson</p>
</header>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#introduction" id="toc-introduction">Introduction</a></li>
<li><a href="#what-are-large-language-models-llms"
id="toc-what-are-large-language-models-llms">What are Large Language
Models (LLMs)?</a>
<ul>
<li><a href="#terminology-in-large-language-models-llms"
id="toc-terminology-in-large-language-models-llms">Terminology in Large
Language Models (LLMs)</a></li>
<li><a href="#deep-learning-transformers-and-generative-language-models"
id="toc-deep-learning-transformers-and-generative-language-models">Deep
Learning, Transformers, and Generative Language Models</a></li>
<li><a href="#the-gpt-architecture" id="toc-the-gpt-architecture">The
GPT Architecture</a></li>
</ul></li>
<li><a href="#llm-abilities" id="toc-llm-abilities">LLM Abilities</a>
<ul>
<li><a href="#gpt-3-abilities-and-evolution"
id="toc-gpt-3-abilities-and-evolution">GPT-3 Abilities and
Evolution</a></li>
<li><a href="#llms-and-knowledge-lookup"
id="toc-llms-and-knowledge-lookup">LLMs and Knowledge Lookup</a></li>
<li><a href="#llms-and-reasoning-abilities"
id="toc-llms-and-reasoning-abilities">LLMs and Reasoning
Abilities</a></li>
<li><a href="#in-context-learning-1"
id="toc-in-context-learning-1">In-Context Learning</a></li>
<li><a href="#emergent-abilities-of-language-models"
id="toc-emergent-abilities-of-language-models">Emergent Abilities of
Language Models</a></li>
<li><a href="#chain-of-thought" id="toc-chain-of-thought">Chain of
Thought</a></li>
<li><a href="#decomposed-prompting"
id="toc-decomposed-prompting">Decomposed Prompting</a></li>
</ul></li>
<li><a href="#why-are-large-language-models-compelling"
id="toc-why-are-large-language-models-compelling">Why Are Large Language
Models Compelling?</a>
<ul>
<li><a href="#tectonic-plate-shifts-in-technology"
id="toc-tectonic-plate-shifts-in-technology">Tectonic Plate Shifts in
Technology</a></li>
<li><a href="#natural-language-as-a-driver-for-any-application"
id="toc-natural-language-as-a-driver-for-any-application">Natural
Language as a Driver for Any Application</a></li>
<li><a
href="#you-dont-have-to-re-train-the-foundation-models-to-do-things"
id="toc-you-dont-have-to-re-train-the-foundation-models-to-do-things">You
Don’t Have to Re-Train the Foundation Models to Do Things</a></li>
</ul></li>
<li><a href="#summary" id="toc-summary">Summary</a></li>
</ul>
</nav>
<h1 id="introduction">Introduction</h1>
<p>Purpose of this series:</p>
<blockquote>
<p>To understand what LLMs are</p>
</blockquote>
<p>This space is moving fast and its valuable to get a mental framing on
how to think about LLMs to better understand how to apply them in your
projects and organization</p>
<p>It also sets context for a better understanding of new developments
in LLMs</p>
<p>Our technical series are based on private reports we produce for our
enterprise customers.</p>
<p>The intended audience for this series is:</p>
<blockquote>
<p>Individual researchers, data scientists, and then also enterprise
data teams as well</p>
</blockquote>
<p>Series:</p>
<ul>
<li><a href="intro_to_llms_part_1_terminology.html">Core Concepts and
Terminology in LLMs</a></li>
<li><a href="intro_to_llms_part_2_applications.html">LLMs and Enterprise
Applications</a></li>
<li><a href="intro_to_llms_part_3_model_management.html">Model Training
and Management in LLMs</a></li>
<li><a href="dl_book_appendix_a_ai.html">Appendix A: What is Artificial
Intelligence?</a></li>
</ul>
<h1 id="what-are-large-language-models-llms">What are Large Language
Models (LLMs)?</h1>
<p>todo</p>
<p>https://www.theatlantic.com/technology/archive/2023/01/artificial-intelligence-ai-chatgpt-dall-e-2-learning/672754/</p>
<p>emergent abilities</p>
<p>https://www.quantamagazine.org/the-unpredictable-abilities-emerging-from-large-ai-models-20230316/</p>
<p>LLMs having their “stable diffusion”-moment</p>
<p>https://simonwillison.net/2023/Mar/11/llama/</p>
<h2 id="terminology-in-large-language-models-llms">Terminology in Large
Language Models (LLMs)</h2>
<p>todo</p>
<h3 id="prompts-and-prompt-engineering">Prompts and Prompt
Engineering</h3>
<p>todo</p>
<p>https://exchange.scale.com/public/events/llm-prompt-engineering-and-rlhf-history-and-techniques-2023-03-09</p>
<p>https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/</p>
<p>https://github.com/dair-ai/Prompt-Engineering-Guide</p>
<p>https://www.pinecone.io/learn/langchain-prompt-templates/</p>
<p>todo</p>
<h3 id="in-context-learning">In-Context Learning</h3>
<h3 id="indexing">Indexing</h3>
<h3 id="embeddings">Embeddings</h3>
<h2 id="deep-learning-transformers-and-generative-language-models">Deep
Learning, Transformers, and Generative Language Models</h2>
<p>todo</p>
<p>https://amatriain.net/blog/transformer-models-an-introduction-and-catalog-2d1e9039f376/</p>
<h3 id="defining-deep-learning">Defining Deep Learning</h3>
<ul>
<li>DL is about automated feature learning</li>
</ul>
<h3 id="introduction-of-transformers-in-2017">Introduction of
Transformers in 2017</h3>
<p>todo</p>
<ul>
<li>dont need to create features</li>
<li>can use tons of data</li>
<li>can use tons of parameters</li>
</ul>
<p>LLMs are implemented with Neural Networks using the Transformer
architecture Transformers were introduced in 2017 they are an
architecture of deep learning neural networks that worked well in
natural language applications</p>
<p>Transformers can process long sequences of text and generate
high-quality outputs for various language tasks, such as * text
generation * question answering * translation</p>
<p>Noteworthy because:</p>
<p>Deep Learning is about using the correct architecture for a data type
such that the architecture can perform automated feature engineering
Transformers are a Deep Learning architecture for natural language
Therefore, Transformer-based systems require no manual feature
engineering</p>
<p>Transformers and LLMs rely directly on raw natural language text as
the input</p>
<h3 id="generative-language-models">Generative Language Models</h3>
<p>todo</p>
<ul>
<li>refernece our strata talk in 2016</li>
</ul>
<h2 id="the-gpt-architecture">The GPT Architecture</h2>
<ul>
<li>based on transformers</li>
<li>doesnt need feature engineering</li>
<li>just give it structured set of natural language</li>
</ul>
<h3 id="gpt-series-of-models">GPT Series of Models</h3>
<p>https://platform.openai.com/docs/models</p>
<h3 id="chatgpt">ChatGPT</h3>
<p>https://help.openai.com/en/articles/6783457-what-is-chatgpt</p>
<pre><code>ChatGPT is fine-tuned from GPT-3.5, a language model trained to produce text. ChatGPT was optimized for dialogue by using Reinforcement Learning with Human Feedback (RLHF) – a method that uses human demonstrations and preference comparisons to guide the model toward desired behavior.</code></pre>
<h3 id="training-data-for-gpt">Training Data for GPT</h3>
<p>https://arxiv.org/abs/2005.14165</p>
<h1 id="llm-abilities">LLM Abilities</h1>
<h2 id="gpt-3-abilities-and-evolution">GPT-3 Abilities and
Evolution</h2>
<p>https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1</p>
<p>Yao Fu writes about the 3 important abilities that the initial GPT-3
exhibit:</p>
<ul>
<li><strong>Language generation</strong>: to follow a prompt and then
generate a completion of the given prompt. Today, this might be the most
ubiquitous way of human-LM interaction.</li>
<li><strong>In-context learning</strong>: to follow a few examples of a
given task and then generate the solution for a new test case. It is
interesting to note that, although being a language model, the original
GPT-3 paper barely talks about “language modeling” — the authors devoted
their writing efforts to their visions of in-context learning, which is
the real focus of GPT-3.</li>
<li><strong>World knowledge</strong>: including factual knowledge and
commonsense.</li>
</ul>
<p>Fu goes on to describe the origin of the abilites of GPT-3:</p>
<blockquote>
<p>Generally, the above three abilities should come from large-scale
pretraining — to pretrain the 175B parameters model on 300B tokens</p>
</blockquote>
<ul>
<li>60% 2016 - 2019 Common Crawl</li>
<li>22% WebText2</li>
<li>16% Books</li>
<li>3% Wikipedia).</li>
</ul>
<p>Where:</p>
<ul>
<li>The <strong>language generation</strong> ability comes from the
language modeling <strong>training objective</strong>.</li>
<li>The <strong>world knowledge</strong> comes from the 300B token
<strong>training corpora</strong> (or where else it could be).</li>
<li>The <strong>175B model size</strong> is for <strong>storing
knowledge</strong>, which is further evidenced by Liang et al. (2022),
who conclude that the performance on tasks requiring knowledge
correlates with model size.</li>
<li>The source of the <strong>in-context learning</strong> ability, as
well as its generalization behavior, <strong>is still elusive</strong>.
Intuitively, this ability may come from the fact that data points of the
same task are ordered sequentially in the same batch during pretraining.
Yet there is little study on why language model pretraining induces
in-context learning, and why in-context learning behaves so differently
than fine-tuning.</li>
</ul>
<!--
Yao Fu then goes on to offer a diagram (embedded below) showing the evolution tree of GPT-3

![GPT Abilities](https://yaofu.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F2ea67e8e-18e1-42d0-9bd9-dbb1f47e22f0%2FUntitled.png?id=a039705a-5a32-41ef-bfb7-1c2793e90e74&table=block&spaceId=281b3c78-d734-43fb-aa33-707babff9463&width=1150&userId=&cache=v2)

Some abilities of note in the above diagram:
-->
<p>For reference, if we say the average book has 80,000 words in it, and
~2 million tokens is roughly the equivalent to 1.5 million words, we can
calculate the total books GPT-3 was trained on to be <strong>around 2.8
million books</strong> based on a 300 billion token training
corpora.</p>
<p>For comparison, the average person might read around 700 books in
their lifetime.</p>
<p>The entire series is wonderful for insight into how large language
models such as GPT-3 get certain types of “abilities” such as “code
generation” and “in-context learning”.</p>
<h2 id="llms-and-knowledge-lookup">LLMs and Knowledge Lookup</h2>
<ul>
<li>define knowledge</li>
<li>explain use in people and in LLMs</li>
<li>discuss specific abilities of GPT-3</li>
</ul>
<h2 id="llms-and-reasoning-abilities">LLMs and Reasoning Abilities</h2>
<ul>
<li>define reasoning</li>
<li>explain and contrast reasoning vs knowledge</li>
<li>discuss specific abilities of GPT-3</li>
</ul>
<p>More Yao Fu:</p>
<blockquote>
<p>Now let’s look at code-davinci-002 and text-davinci-002, the two
first GPT3.5 models, one for code and the other for text. There are four
important abilities they exhibit that differentiate them from the
initial GPT-3</p>
</blockquote>
<ul>
<li><strong>Responding to human instruction</strong>: previously, the
outputs of GPT-3 were mostly high-frequency prompt-completion patterns
within the training set. Now the model generates reasonable answers to
the prompt, rather than related but useless sentences.</li>
<li><strong>Generalization to unseen tasks</strong>: when the number of
instructions used for tuning the model is beyond a certain scale, the
model can automatically generate completions for new instructions that
are not in the training set. <strong>This ability is crucial for
deployment</strong>, as users with always come up with new prompts.</li>
<li><strong>Code generation and code understanding</strong>: obviously,
because the model is trained on code.</li>
<li><strong>Complex reasoning</strong> <strong>with
chain-of-thought</strong></li>
</ul>
<h2 id="in-context-learning-1">In-Context Learning</h2>
<p>todo</p>
<p>https://datascience.stackexchange.com/questions/115554/how-exactly-does-in-context-few-shot-learning-actually-work-in-theory-under-the</p>
<p>https://arxiv.org/pdf/2005.14165.pdf</p>
<p>https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00485/111728/True-Few-Shot-Learning-with-Prompts-A-Real-World</p>
<p>http://ai.stanford.edu/blog/understanding-incontext/</p>
<h2 id="emergent-abilities-of-language-models">Emergent Abilities of
Language Models</h2>
<p>https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1</p>
<h2 id="chain-of-thought">Chain of Thought</h2>
<h2 id="decomposed-prompting">Decomposed Prompting</h2>
<p>[ICLR 2023] Decomposed Prompting: A Modular Approach for Solving
Complex Tasks. [paper][code]</p>
<h1 id="why-are-large-language-models-compelling">Why Are Large Language
Models Compelling?</h1>
<p>todo</p>
<h2 id="tectonic-plate-shifts-in-technology">Tectonic Plate Shifts in
Technology</h2>
<p>todo</p>
<h2 id="natural-language-as-a-driver-for-any-application">Natural
Language as a Driver for Any Application</h2>
<p>toodo</p>
<p>ChatGPT was a nice demo, but there is a lot more here</p>
<h2
id="you-dont-have-to-re-train-the-foundation-models-to-do-things">You
Don’t Have to Re-Train the Foundation Models to Do Things</h2>
<p>It’s amazing</p>
<h1 id="summary">Summary</h1>
<p>https://cube.dev/blog/conversational-interface-for-semantic-layer</p>
<p>https://github.com/approximatelabs/sketch</p>
</body>
</html>
