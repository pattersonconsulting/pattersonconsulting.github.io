<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Josh Patterson" />
  <meta name="keywords" content="snowflake, snowpark, automl, AutoGluon, pandas, dataframe, whl, pip, anaconda, dependency" />
  <title>An Introduction to Large Language Models (LLMs)</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header>
<h1 class="title">An Introduction to Large Language Models (LLMs)</h1>
<p class="subtitle">Part 3 - Putting LLM Applications into Production</p>
<p class="author">Josh Patterson</p>
</header>
<nav id="TOC">
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#which-model-should-i-use-for-my-llm-application">Which Model Should I Use for My LLM Application?</a><ul>
<li><a href="#api-vs-in-house-models">API vs In-House Models</a></li>
<li><a href="#the-cost-of-cloud-apis-at-scale">The Cost of Cloud APIs at Scale</a></li>
</ul></li>
<li><a href="#how-are-llm-models-customized-or-trained">How are LLM Models Customized or Trained?</a><ul>
<li><a href="#in-context-learning">In-Context Learning</a></li>
<li><a href="#fine-tuninng">Fine-Tuninng</a></li>
<li><a href="#rhlf-other-stuff">RHLF — other stuff</a></li>
<li><a href="#llm-training-data">LLM Training Data</a></li>
<li><a href="#the-impact-of-parameter-sizes-in-llms">The Impact of Parameter Sizes in LLMs</a></li>
</ul></li>
<li><a href="#other-considerations-for-deploying-large-language-model-applications">Other Considerations for Deploying Large Language Model Applications</a><ul>
<li><a href="#closed-source-vs-open-source-models">Closed Source vs Open Source Models</a></li>
<li><a href="#trends-in-model-parameter-sizes">Trends in Model Parameter Sizes</a></li>
<li><a href="#specializing-smaller-models-for-specific-performance">Specializing Smaller Models for Specific Performance</a></li>
</ul></li>
<li><a href="#summary">Summary</a></li>
</ul>
</nav>
<h1 id="introduction">Introduction</h1>
<p>Purpose of this series:</p>
<blockquote>
<p>To understand what LLMs are</p>
</blockquote>
<p>The intended audience for this series is:</p>
<blockquote>
<p>Individual researchers, data scientists, and then also enterprise data teams as well</p>
</blockquote>
<p>Series:</p>
<ul>
<li><a href="intro_to_llms_part_1_terminology.html">Background and Core Concepts in LLMs</a></li>
<li><a href="intro_to_llms_part_2_applications.html">A Guide for Building Enterprise Application with LLMs</a></li>
<li><a href="intro_to_llms_part_3_model_management.html">Putting LLM Applications into Production</a></li>
<li><a href="dl_book_appendix_a_ai.html">Appendix A: What is Artificial Intelligence?</a></li>
</ul>
<h1 id="which-model-should-i-use-for-my-llm-application">Which Model Should I Use for My LLM Application?</h1>
<p>todo</p>
<p>Numbers every LLM developer should know:</p>
<p>https://github.com/ray-project/llm-numbers</p>
<h2 id="api-vs-in-house-models">API vs In-House Models</h2>
<p>asdasd</p>
<h2 id="the-cost-of-cloud-apis-at-scale">The Cost of Cloud APIs at Scale</h2>
<p>link work here</p>
<p>https://huyenchip.com/2023/04/11/llm-engineering.html</p>
<pre><code>The more explicit detail and examples you put into the prompt, the better the model performance (hopefully), and the more expensive your inference will cost.
OpenAI API charges for both the input and output tokens. Depending on the task, a simple prompt might be anything between 300 - 1000 tokens. If you want to include more context, e.g. adding your own documents or info retrieved from the Internet to the prompt, it can easily go up to 10k tokens for the prompt alone.
The cost with long prompts isn’t in experimentation but in inference.

Experimentation-wise, prompt engineering is a cheap and fast way get something up and running. For example, even if you use GPT-4 with the following setting, your experimentation cost will still be just over $300. The traditional ML cost of collecting data and training models is usually much higher and takes much longer.
* Prompt: 10k tokens ($0.06/1k tokens)
* Output: 200 tokens ($0.12/1k tokens)
* Evaluate on 20 examples
* Experiment with 25 different versions of prompts
</code></pre>
<pre><code>The cost of LLMOps is in inference.
* If you use GPT-4 with 10k tokens in input and 200 tokens in output, it’ll be $0.624 / prediction.
* If you use GPT-3.5-turbo with 4k tokens for both input and output, it’ll be $0.004 / prediction or $4 / 1k predictions.
* As a thought exercise, in 2021, DoorDash ML models made 10 billion predictions a day. 
   * If each prediction costs $0.004, that’d be $40 million a day!
* By comparison, AWS personalization costs about $0.0417 / 1k predictions and AWS fraud detection costs about $7.5 / 1k predictions [for over 100,000 predictions a month]. AWS services are usually considered prohibitively expensive (and less flexible) for any company of a moderate scale.

</code></pre>
<h1 id="how-are-llm-models-customized-or-trained">How are LLM Models Customized or Trained?</h1>
<p>now this is an interesting topic!</p>
<h2 id="in-context-learning">In-Context Learning</h2>
<h2 id="fine-tuninng">Fine-Tuninng</h2>
<h3 id="when-should-i-fine-tune-a-model">When Should I Fine-Tune a Model?</h3>
<p>Currently there is no great heuristic here for the threshold to fine-tune over using advanced context mechanics Does using Context Mechanics give you a “good enough” result for the current problem? if no, then it may be worth experimenting with fine-tuning</p>
<p>Context Some papers suggest you need around 50K examples in order to improve alignment. So if you have that many examples maybe it’s worth it. Fine-tuning needs to improve “alignment” ultimately for any use case I think in the context of LLMs the purpose of “fine tuning” is really “alignment”; that is to better align model outputs with the intended task.</p>
<h2 id="rhlf-other-stuff">RHLF — other stuff</h2>
<h2 id="llm-training-data">LLM Training Data</h2>
<p>Training Data for Stanford Alpaca</p>
<p>https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json</p>
<h2 id="the-impact-of-parameter-sizes-in-llms">The Impact of Parameter Sizes in LLMs</h2>
<p>there are light weight models that are approaching the quality of chat GPT.<br />
It’s a space to watch.<br />
There is reason to believe this will all be very accessible on consumer grade hardware soon</p>
<h3 id="gpu-memory-as-upper-ceiling">GPU Memory as Upper Ceiling</h3>
<p>1GB == 1B paramters</p>
<h1 id="other-considerations-for-deploying-large-language-model-applications">Other Considerations for Deploying Large Language Model Applications</h1>
<p>todo</p>
<h2 id="closed-source-vs-open-source-models">Closed Source vs Open Source Models</h2>
<ul>
<li>todo</li>
</ul>
<h2 id="trends-in-model-parameter-sizes">Trends in Model Parameter Sizes</h2>
<ul>
<li>todo</li>
</ul>
<h2 id="specializing-smaller-models-for-specific-performance">Specializing Smaller Models for Specific Performance</h2>
<p>[ICML 2023] Specializing Smaller Language Models towards Multi-Step Reasoning.</p>
<p>https://arxiv.org/abs/2301.12726</p>
<pre><code>We show that such abilities can, in fact, be distilled down from GPT3.5 (≥ 175B) to T5 variants (≤ 11B)

. We propose
model specialization, to specialize the model’s
ability towards a target task. The hypothesis is
that large models (commonly viewed as larger
than 100B) have strong modeling power, but are
spread on a large spectrum of tasks. Small models (commonly viewed as smaller than 10B) have
limited model capacity, but if we concentrate their
capacity on a specific target task, the model can
achieve a decent improved performance</code></pre>
<p>https://github.com/FranxYao/FlanT5-CoT-Specialization</p>
<p>Smaller models performing</p>
<p>https://albertoromgar.medium.com/the-era-of-large-ai-models-is-over-a5c9d7d804d4</p>
<pre><code>Don’t conflate the reduced size of these models with them being inferior performance-wise: LLaMA 13B benchmark results are comparable to GPT-3 175B despite the latter being 13x larger. Three years have sufficed to improve the state-of-the-art on variables other than size (e.g., finetuning techniques, data quality, or hardware-software optimizations) so that the largest models of 2020 will be eventually dwarfed by the efficiency of much smaller new ones.

https://twitter.com/GuillaumeLample/status/1629151231800115202</code></pre>
<h1 id="summary">Summary</h1>
</body>
</html>
